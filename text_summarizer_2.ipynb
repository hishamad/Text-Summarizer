{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: terminaltables in /opt/conda/lib/python3.10/site-packages (3.1.10)\n"
     ]
    }
   ],
   "source": [
    "# First run this cell\n",
    "!pip install terminaltables rouge\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from terminaltables import AsciiTable\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "data = data.loc[:, ['Summary', 'Text']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)\n",
    "\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_split = 0.8\n",
    "test_split = 1-train_split\n",
    "\n",
    "train_data = data[:int(len(data)*train_split)].reset_index(drop=True)\n",
    "test_data = data[int(len(data)*train_split):].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_w2i = {}\n",
    "source_i2w = []\n",
    "target_w2i = {}\n",
    "target_i2w = []\n",
    "\n",
    "# The padding symbol will be used to ensure that all tensors in a batch\n",
    "# have equal length.\n",
    "PADDING_SYMBOL = ' '\n",
    "source_w2i[PADDING_SYMBOL] = 0\n",
    "source_i2w.append(PADDING_SYMBOL)\n",
    "target_w2i[PADDING_SYMBOL] = 0\n",
    "target_i2w.append(PADDING_SYMBOL)\n",
    "\n",
    "START_SYMBOL = '<START>'\n",
    "END_SYMBOL = '<END>'\n",
    "UNK_SYMBOL = '<UNK>'\n",
    "source_w2i[START_SYMBOL] = 1\n",
    "source_i2w.append(START_SYMBOL)\n",
    "target_w2i[START_SYMBOL] = 1\n",
    "target_i2w.append(START_SYMBOL)\n",
    "source_w2i[END_SYMBOL] = 2\n",
    "source_i2w.append(END_SYMBOL)\n",
    "target_w2i[END_SYMBOL] = 2\n",
    "target_i2w.append(END_SYMBOL)\n",
    "source_w2i[UNK_SYMBOL] = 3\n",
    "source_i2w.append(UNK_SYMBOL)\n",
    "target_w2i[UNK_SYMBOL] = 3\n",
    "target_i2w.append(UNK_SYMBOL)\n",
    "\n",
    "# Max number of words to be predicted if <END> symbol is not reached\n",
    "MAX_PREDICTIONS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataset(Dataset) :\n",
    "    \"\"\"\n",
    "    A dataset with source sentences and their respective translations\n",
    "    into the target language.\n",
    "\n",
    "    Each sentence is represented as a list of word IDs. \n",
    "    \"\"\"\n",
    "    def __init__(self, data, record_symbols=True):\n",
    "        try:\n",
    "            nltk.word_tokenize(\"hi there.\")\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "        self.source_list = []\n",
    "        self.target_list = []\n",
    "        # Read the datafile\n",
    "        \n",
    "        for i in tqdm(range(len(data))):\n",
    "            s = data.Text[i]\n",
    "            t = data.Summary[i]\n",
    "            source_sentence = []\n",
    "            for w in nltk.word_tokenize(s):\n",
    "                w = w.lower()\n",
    "                if w not in source_i2w and record_symbols:\n",
    "                    source_w2i[w] = len(source_i2w)\n",
    "                    source_i2w.append(w)\n",
    "                source_sentence.append(source_w2i.get(w, source_w2i[UNK_SYMBOL]))\n",
    "            source_sentence.append(source_w2i[END_SYMBOL])\n",
    "            self.source_list.append(source_sentence)\n",
    "            target_sentence = []\n",
    "            for w in nltk.word_tokenize(t):\n",
    "                w = w.lower()\n",
    "                if w not in target_i2w and record_symbols:\n",
    "                    target_w2i[w] = len(target_i2w)\n",
    "                    target_i2w.append(w)\n",
    "                target_sentence.append(target_w2i.get(w, target_w2i[UNK_SYMBOL]))\n",
    "            target_sentence.append(target_w2i[END_SYMBOL])\n",
    "            self.target_list.append(target_sentence)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_list[idx], self.target_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454728/454728 [19:22<00:00, 391.31it/s]\n",
      "100%|██████████| 113683/113683 [05:30<00:00, 344.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AmazonDataset(train_data)\n",
    "test_dataset = AmazonDataset(test_data, record_symbols=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'data/amazon_train_dataset_py')\n",
    "torch.save(test_dataset, 'data/amazon_test_dataset_py')\n",
    "\n",
    "with open('source_w2i.json', 'w') as f:\n",
    "    f.write(json.dumps(source_w2i))\n",
    "\n",
    "with open('source_i2w.json', 'w') as f:\n",
    "    f.write(json.dumps(source_i2w))\n",
    "\n",
    "with open('target_w2i.json', 'w') as f:\n",
    "    f.write(json.dumps(target_w2i))\n",
    "\n",
    "with open('target_i2w.json', 'w') as f:\n",
    "    f.write(json.dumps(target_i2w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load('data/amazon_train_dataset_py')\n",
    "test_dataset = torch.load('data/amazon_test_dataset_py')\n",
    "\n",
    "source_w2i = {}\n",
    "source_i2w = []\n",
    "target_w2i = {}\n",
    "target_i2w = []\n",
    "\n",
    "with open('source_w2i.json', 'r') as f:\n",
    "    source_w2i = json.load(f)\n",
    "\n",
    "with open('source_i2w.json', 'r') as f:\n",
    "    source_i2w = json.load(f)\n",
    "\n",
    "with open('target_w2i.json', 'r') as f:\n",
    "    target_w2i = json.load(f)\n",
    "\n",
    "with open('target_i2w.json', 'r') as f:\n",
    "    target_i2w = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch, pad_source=source_w2i[PADDING_SYMBOL], pad_target=target_w2i[PADDING_SYMBOL]):\n",
    "    source, target = zip(*batch)\n",
    "    max_source_len = max(map(len, source))\n",
    "    max_target_len = max(map(len, target))\n",
    "    padded_source = [[b[i] if i < len(b) else pad_source for i in range(max_source_len)] for b in source]\n",
    "    padded_target = [[l[i] if i < len(l) else pad_target for i in range(max_target_len)] for l in target]\n",
    "    return padded_source, padded_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(embedding_file) :\n",
    "    \"\"\"\n",
    "    Reads pre-made embeddings from a file\n",
    "    \"\"\"\n",
    "    N = len(source_w2i)\n",
    "    embeddings = [0]*N\n",
    "    with codecs.open(embedding_file, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0].lower()\n",
    "            if word not in source_w2i:\n",
    "                source_w2i[word] = N\n",
    "                source_i2w.append(word)\n",
    "                N += 1\n",
    "                embeddings.append(0)\n",
    "            vec = [float(x) for x in data[1:]]\n",
    "            D = len(vec)\n",
    "            embeddings[source_w2i[word]] = vec\n",
    "    # Add a '0' embedding for the padding symbol\n",
    "    embeddings[0] = [0]*D\n",
    "    # Check if there are words that did not have a ready-made Glove embedding\n",
    "    # For these words, add a random vector\n",
    "    for word in source_w2i :\n",
    "        index = source_w2i[word]\n",
    "        if embeddings[index] == 0 :\n",
    "            embeddings[index] = (np.random.random(D)-0.5).tolist()\n",
    "    return D, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Encoder ==================== #\n",
    "\n",
    "class EncoderRNN(nn.Module) :\n",
    "    \"\"\"\n",
    "    Encodes a batch of source sentences. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, no_of_input_symbols, embeddings=None, embedding_size=16, hidden_size=25,\n",
    "        encoder_bidirectional=False, device='cpu', use_gru=False, tune_embeddings=False) :\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.is_bidirectional = encoder_bidirectional\n",
    "        self.embedding = nn.Embedding(no_of_input_symbols,embedding_size)\n",
    "        if embeddings !=  None:\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(embeddings, dtype=torch.float), requires_grad=tune_embeddings)\n",
    "        if use_gru:\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True, bidirectional=self.is_bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=self.is_bidirectional)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def set_embeddings(self, embeddings):\n",
    "        self.embedding.weight = torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a list of lists of size (batch_size,max_seq_length)\n",
    "        Each inner list contains word IDs and represents one sentence.\n",
    "        The whole list-of-lists represents a batch of sentences.\n",
    "       \n",
    "        Returns:\n",
    "        the output from the encoder RNN: a pair of two tensors, one containing all hidden states, and one \n",
    "        containing the last hidden state (see https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "        \"\"\"\n",
    "        x_tensor = torch.tensor(x).to(self.device)\n",
    "        embedded_words = self.embedding(x_tensor)\n",
    "        all_hidden, last_hidden = self.rnn(embedded_words)\n",
    "        return all_hidden, last_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Decoder ==================== #\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, no_of_output_symbols, embedding_size=16, hidden_size=25, use_attention=True,\n",
    "        display_attention=False, device='cpu', use_gru=False) :\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(no_of_output_symbols,embedding_size)\n",
    "        self.no_of_output_symbols = no_of_output_symbols\n",
    "        self.W = nn.Parameter(torch.rand(hidden_size, hidden_size)-0.5) # shouldn't W be 2*hidden_size\n",
    "        self.U = nn.Parameter(torch.rand(hidden_size, hidden_size)-0.5)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size, 1)-0.5)\n",
    "        self.use_attention = use_attention\n",
    "        self.display_attention = display_attention\n",
    "        if use_gru:\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_size, no_of_output_symbols)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        'input' is a list of length batch_size, containing the current word\n",
    "        of each sentence in the batch\n",
    "\n",
    "        'hidden' is a tensor containing the last hidden state of the decoder, \n",
    "        for each sequence in the batch\n",
    "        hidden.shape = (1, batch_size, hidden_size)\n",
    "\n",
    "        'encoder_outputs' is a tensor containing all hidden states from the\n",
    "        encoder (used in problem c)\n",
    "        encoder_outputs.shape = (batch_size, max_seq_length, hidden_size)\n",
    "\n",
    "        Note that 'max_seq_length' above refers to the max_seq_length\n",
    "        of the encoded sequence (not the decoded sequence).\n",
    "\n",
    "        Returns:\n",
    "        If use_attention and display_attention are both True (task (c)), return a triple\n",
    "        (logits for the predicted next word, hidden state, attention weights alpha)\n",
    "\n",
    "        Otherwise (task (b)), return a pair\n",
    "        (logits for the predicted next word, hidden state).\n",
    "        \"\"\"\n",
    "        \n",
    "        inp_tensor = torch.tensor(inp).to(self.device)\n",
    "        word_embs = self.embedding(inp_tensor).unsqueeze(1)\n",
    "\n",
    "        if not self.use_attention:\n",
    "            rnn_output, hidden = self.rnn(word_embs, hidden)\n",
    "            logits = self.output(rnn_output.squeeze(1))\n",
    "            return logits, hidden\n",
    "        \n",
    "        context, alpha_ij = self.get_context(word_embs, encoder_outputs)\n",
    "        rnn_output, hidden = self.rnn(word_embs, context)\n",
    "        logits = self.output(rnn_output.squeeze(1))\n",
    "\n",
    "        if self.display_attention:\n",
    "            return logits, hidden, alpha_ij\n",
    "        return logits, hidden\n",
    "\n",
    "    def get_context(self, prev_word_embs, encoder_states):\n",
    "        summed = (torch.matmul(prev_word_embs, self.U) + torch.matmul(encoder_states, self.W))\n",
    "        summed = torch.tanh(summed)\n",
    "        e_ij = torch.matmul(summed, self.v)\n",
    "        alpha_ij = torch.softmax(e_ij, dim=1)\n",
    "        context = alpha_ij * encoder_states\n",
    "        context = context.sum(dim=1).unsqueeze(0)\n",
    "        return context, alpha_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ds, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    num_correct_words = 0\n",
    "    num_correct_sentences = 0\n",
    "    \n",
    "    tot_words = 0\n",
    "    tot_sentances = 0\n",
    "\n",
    "    predicted_sentences = []\n",
    "    correct_sentences = []\n",
    "    \n",
    "    #for x, y in tqdm(ds):\n",
    "    for x, y in ds:\n",
    "        predicted_sentence = []\n",
    "        with torch.no_grad():\n",
    "            outputs, hidden = encoder([x])\n",
    "        if encoder.is_bidirectional:\n",
    "            hidden = hidden.permute((1,0,2)).reshape(1,-1).unsqueeze(0)\n",
    "        \n",
    "        predicted_symbol = target_w2i[START_SYMBOL]\n",
    "        predicted_sentence = []\n",
    "        num_attempts = 0\n",
    "        while num_attempts < MAX_PREDICTIONS:\n",
    "            with torch.no_grad():\n",
    "                predictions, hidden = decoder([predicted_symbol], hidden, outputs)    \n",
    "            _, predicted_tensor = predictions.topk(1)\n",
    "            predicted_symbol = predicted_tensor.detach().item()\n",
    "    \n",
    "            num_attempts += 1\n",
    "    \n",
    "            if predicted_symbol == target_w2i[END_SYMBOL]:\n",
    "                break\n",
    "                \n",
    "            predicted_sentence.append(predicted_symbol)\n",
    "\n",
    "        # [:-1] such we dont consider the end symbol\n",
    "        y = y[:-1]\n",
    "        \n",
    "        if predicted_sentence == y:\n",
    "            num_correct_sentences += 1\n",
    "\n",
    "        for w_p, w_y in zip(predicted_sentence, y):\n",
    "            if w_p == w_y:\n",
    "                num_correct_words += 1\n",
    "\n",
    "        tot_words += len(y)\n",
    "        tot_sentances += 1\n",
    "\n",
    "        predicted_sentence_str = \" \".join([target_i2w[i] for i in predicted_sentence])\n",
    "        correct_sentence_str = \" \".join([source_i2w[i] for i in y])\n",
    "\n",
    "        #if len(predicted_sentence_str) > 1 and len(correct_sentence_str) > 1:\n",
    "        predicted_sentences.append(predicted_sentence_str)\n",
    "        correct_sentences.append(correct_sentence_str)\n",
    "\n",
    "    print(predicted_sentences[-1])\n",
    "    print(correct_sentences[-1])\n",
    "\n",
    "    #rouge = Rouge()\n",
    "    #print(rouge.get_scores(predicted_sentences, correct_sentences, avg=True))  \n",
    "\n",
    "    word_acc = num_correct_words / tot_words\n",
    "    sent_acc = num_correct_sentences / tot_sentances\n",
    "\n",
    "    print(f\"Word acc: {word_acc*100:.2f}%\")\n",
    "    print(f\"Sent acc: {sent_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Hyper-parameters ================ #\n",
    "use_attention = True     \n",
    "use_gru = True         # Use Gated Recurrent Units (rather than plain RNNs)\n",
    "bidirectional = True   # Use a bidirectional encoder\n",
    "use_embeddings = True  # Use pre-loaded Glove embeddings\n",
    "tune_embeddings = True # Fine-tune the Glove embeddings\n",
    "batch_size = 64\n",
    "hidden_size = 25       # Number of dimensions in the hidden state\n",
    "learning_rate = 0.001\n",
    "epochs = 50            # We will train for this many epochs\n",
    "save = False           # Do not save the model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_sequence)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size, embeddings = load_glove_embeddings('/datasets/dd2417/glove.6B.50d.txt')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "    len(source_i2w),\n",
    "    embeddings=embeddings,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    encoder_bidirectional=bidirectional,\n",
    "    tune_embeddings=tune_embeddings,\n",
    "    use_gru=use_gru,\n",
    "    device=device\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    len(target_i2w),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size*(bidirectional+1),\n",
    "    use_attention=use_attention,\n",
    "    use_gru=use_gru,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:16:45 Starting training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7106/7106 [03:40<00:00, 32.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:20:25 Epoch 0 loss: 204.9815216064453\n",
      "Evaluating on the test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 32040/113683 [01:03<02:36, 522.46it/s]"
     ]
    }
   ],
   "source": [
    "print(datetime.now().strftime(\"%H:%M:%S\"), \"Starting training.\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for source, target in tqdm(train_loader):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        # hidden is (D * num_layers, B, H)\n",
    "        outputs, hidden = encoder(source)\n",
    "        if bidirectional:\n",
    "            # (2, B, H) -> (B, 2 * H) -> (1, B, 2 * H)\n",
    "            hidden = torch.cat([hidden[0,:, :], hidden[1,:,:]], dim=1).unsqueeze(0)\n",
    "                    \n",
    "        # The probability of doing teacher forcing will decrease\n",
    "        # from 1 to 0 over the range of epochs. This could be implemented\n",
    "        # like this:\n",
    "        # teacher_forcing_ratio = 1- epoch/args.epochs\n",
    "        # But, for now we will always use teacher forcing\n",
    "        teacher_forcing_ratio = 1\n",
    "\n",
    "        # The input to the decoder in the first time step will be\n",
    "        # the boundary symbol, regardless if we are using teacher\n",
    "        # forcing or not.\n",
    "        idx = [target_w2i[START_SYMBOL] for sublist in target]\n",
    "        predicted_symbol = [target_w2i[START_SYMBOL] for sublist in target]\n",
    "\n",
    "        target_length = len(target[0])\n",
    "        for i in range(target_length) :\n",
    "            use_teacher_forcing = (random.random() < teacher_forcing_ratio)\n",
    "            if use_teacher_forcing :\n",
    "                predictions, hidden = decoder(idx, hidden, outputs)\n",
    "            else:\n",
    "                # Here we input the previous prediction rather than the\n",
    "                # correct symbol.\n",
    "                predictions, hidden = decoder(predicted_symbol, hidden, outputs)\n",
    "            _, predicted_tensor = predictions.topk(1)\n",
    "            predicted_symbol = predicted_tensor.squeeze().tolist()\n",
    "\n",
    "            # The targets will be the ith symbol of all the target\n",
    "            # strings. They will also be used as inputs for the next\n",
    "            # time step if we use teacher forcing.\n",
    "            idx = [sublist[i] for sublist in target]\n",
    "            loss += criterion(predictions.squeeze(), torch.tensor(idx).to(device))\n",
    "        loss /= (target_length * batch_size)\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"), \"Epoch\", epoch, \"loss:\", total_loss.detach().item())\n",
    "    total_loss = 0\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(\"Evaluating on the test data...\")\n",
    "        evaluate(test_dataset, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/113683 [00:00<03:20, 566.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my 3-year old akita is very picky when it comes to food , and usually refuses most of the standard dog treats . this is the only thing he visibly appreciates ; btw , when offered chicken happy hips of the same brand he 's not as interested. < br / > < br / > the only downside is the price ; do n't ever buy it in retail stores - they charge 2-3 times more than online retailers . even then it 's rather expensive. < br / > < br / > hope this helps . <END>\n",
      "\n",
      "Y-TRUE: the only thing my dog will eat <END>\n",
      "Y-PRED: my dog loves these treats <END>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "to put is simply , it ruins you for other <UNK> < br / > < br / > we tried some of this on a whim from our local costco once . it is amazing . even though there is great care needed to pop it just the right amount of time ( the sugary stuff burns quickly ) , it is so very , very worth it . as others have said , it 's like the fresh kind made in a kettle . it 's no sweet flavored popcorn , it actually has the sweet/salty coating on the popped kernels . delicious . much like others , i have bought and split a case with another family before . it was gone in a flash. < br / > < br / > in my opinion , if you 're going to try and eat healthy popcorn to stave off hunger of something , buy the `` healthy choice '' stuff or get a hot air popper . but if you want to truly enjoy popcorn ... this is it . <END>\n",
      "\n",
      "Y-TRUE: best microwave popcorn . not like other `` kettle corns '' <END>\n",
      "Y-PRED: the best ! <END>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "i met my wife in peru in early february of 2007 . i was searching for a soda ( i 've never been a fan of coke or pepsi ) and inca kola looked a lot like mountain dew . it 's not a thing like mountain dew . it 's far sweeter , and far more delicious . in about a week i 'd probably drank 3-4 gallons of it while i was in peru . very very very few places in the states offer inca kola though , and this is a very good price for it . it arrived in excellent condition and very thoughtfully packed from this seller , not only would i heartily recommend inca kola , i 'd also recommend mexigrocer ! <END>\n",
      "\n",
      "Y-TRUE: amazing soda ! <END>\n",
      "Y-PRED: great product <END>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "i am slightly confused by the two previous reviews of this tea , which state that it is not flavoured . i was introduced to this tea by two friends from st. petersburg who gave me an open box during the course of a visit . this is not simply a black tea , it is a spiced tea similar to earl grey . the paradox is that i am nto an earl grey lover - my tea preferences tend more towards assam or lapsang souchong . however for me this rea gets that extra taste just right . it 's very pricy for me to order this for delivery to the uk , but i do that , so take that as a concrete indication of my appreciation . if you like earl grey , try this . <END>\n",
      "\n",
      "Y-TRUE: a better earl grey <END>\n",
      "Y-PRED: not as good as a good price <END>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "very price competitive with health food stores when purchased via amazon ... < br / > another good gluten free item ... <END>\n",
      "\n",
      "Y-TRUE: price .... gluten free ... <END>\n",
      "Y-PRED: great product <END>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "our dogs love this stuff and we keep it in the motorhome and on the boat to eliminate the bulk and waste of cans. < br / > < br / > the only issue i have is that i can buy old roy from walmart for 1/2 the price and it is the exact same <UNK> < br / > < br / > that being said , the box is heavy and they do leave it at the top of my 23 steps to the house . <END>\n",
      "\n",
      "Y-TRUE: dogs love it but too <UNK> <END>\n",
      "Y-PRED: great product <END>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "num_examples = 5\n",
    "\n",
    "count = 0\n",
    "for x, y in test_dataset:\n",
    "    predicted_sentence = []\n",
    "    with torch.no_grad():\n",
    "        outputs, hidden = encoder([x])\n",
    "    if encoder.is_bidirectional:\n",
    "        hidden = hidden.permute((1,0,2)).reshape(1,-1).unsqueeze(0)\n",
    "    predicted_symbol = target_w2i[START_SYMBOL]\n",
    "    predicted_sentence = []\n",
    "    num_attempts = 0\n",
    "    while num_attempts < MAX_PREDICTIONS:\n",
    "        with torch.no_grad():\n",
    "            predictions, hidden = decoder([predicted_symbol], hidden, outputs)    \n",
    "        _, predicted_tensor = predictions.topk(1)\n",
    "        predicted_symbol = predicted_tensor.detach().item()\n",
    "        predicted_sentence.append(predicted_symbol)\n",
    "\n",
    "        num_attempts += 1\n",
    "\n",
    "        if predicted_symbol == target_w2i[END_SYMBOL]:\n",
    "            break\n",
    "    \n",
    "    in_sent = ' '.join([source_i2w[i] for i in x])\n",
    "    y_sent = ' '.join([target_i2w[i] for i in y])\n",
    "    pred_sent = ' '.join([target_i2w[i] for i in predicted_sentence])\n",
    "\n",
    "    print(in_sent)\n",
    "    print()\n",
    "    print(f\"Y-TRUE: {y_sent}\")\n",
    "    print(f\"Y-PRED: {pred_sent}\")\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    count += 1\n",
    "    if count > num_examples:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "63009f7f2cc996fccde2458c7f62140de231233c09c29bb8f0fa7c6864762ba3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
