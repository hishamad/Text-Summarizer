{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: terminaltables in /opt/conda/lib/python3.10/site-packages (3.1.10)\n",
      "Requirement already satisfied: rouge in /opt/conda/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# First run this cell\n",
    "!pip install terminaltables rouge\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from terminaltables import AsciiTable\n",
    "from rouge import Rouge\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = './data/reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/Reviews.csv\")[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "data = data.loc[:, ['Summary', 'Text']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data[:20000]\n",
    "test_data = data[20000:]\n",
    "\n",
    "training_data = training_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_w2i = {}\n",
    "source_i2w = []\n",
    "target_w2i = {}\n",
    "target_i2w = []\n",
    "\n",
    "# The padding symbol will be used to ensure that all tensors in a batch\n",
    "# have equal length.\n",
    "PADDING_SYMBOL = ' '\n",
    "source_w2i[PADDING_SYMBOL] = 0\n",
    "source_i2w.append( PADDING_SYMBOL )\n",
    "target_w2i[PADDING_SYMBOL] = 0\n",
    "target_i2w.append( PADDING_SYMBOL )\n",
    "\n",
    "START_SYMBOL = '<START>'\n",
    "END_SYMBOL = '<END>'\n",
    "UNK_SYMBOL = '<UNK>'\n",
    "source_w2i[START_SYMBOL] = 1\n",
    "source_i2w.append( START_SYMBOL )\n",
    "target_w2i[START_SYMBOL] = 1\n",
    "target_i2w.append( START_SYMBOL )\n",
    "source_w2i[END_SYMBOL] = 2\n",
    "source_i2w.append( END_SYMBOL )\n",
    "target_w2i[END_SYMBOL] = 2\n",
    "target_i2w.append( END_SYMBOL )\n",
    "source_w2i[UNK_SYMBOL] = 3\n",
    "source_i2w.append( UNK_SYMBOL )\n",
    "target_w2i[UNK_SYMBOL] = 3\n",
    "target_i2w.append( UNK_SYMBOL )\n",
    "\n",
    "# Max number of words to be predicted if <END> symbol is not reached\n",
    "MAX_PREDICTIONS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataset(Dataset) :\n",
    "    \"\"\"\n",
    "    A dataset with source sentences and their respective translations\n",
    "    into the target language.\n",
    "\n",
    "    Each sentence is represented as a list of word IDs. \n",
    "    \"\"\"\n",
    "    def __init__( self, data, record_symbols=True ) :\n",
    "        try :\n",
    "            nltk.word_tokenize(\"hi there.\")\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "        self.source_list = []\n",
    "        self.target_list = []\n",
    "        # Read the datafile\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            s = data.Text[i]\n",
    "            t = data.Summary[i]\n",
    "            source_sentence = []\n",
    "            for w in nltk.word_tokenize(s) :\n",
    "                w = w.lower()\n",
    "                if w not in source_i2w and record_symbols :\n",
    "                    source_w2i[w] = len(source_i2w)\n",
    "                    source_i2w.append( w )\n",
    "                source_sentence.append( source_w2i.get(w, source_w2i[UNK_SYMBOL]) )\n",
    "            source_sentence.append(source_w2i[END_SYMBOL])\n",
    "            self.source_list.append( source_sentence )\n",
    "            target_sentence = []\n",
    "            for w in nltk.word_tokenize(t) :\n",
    "                w = w.lower()\n",
    "                if w not in target_i2w and record_symbols :\n",
    "                    target_w2i[w] = len(target_i2w)\n",
    "                    target_i2w.append( w )\n",
    "                target_sentence.append( target_w2i.get(w, target_w2i[UNK_SYMBOL]) )\n",
    "            target_sentence.append(target_w2i[END_SYMBOL])\n",
    "            self.target_list.append( target_sentence )\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.source_list)\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        return self.source_list[idx], self.target_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dataset, 'amazon_dataset_py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = AmazonDataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = AmazonDataset(test_data, record_symbols=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_dataset = torch.load('./data/amazon_dataset_py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(loaded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataset, test_dataset = torch.utils.data.random_split(loaded_dataset, [0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch, pad_source=source_w2i[PADDING_SYMBOL], pad_target=target_w2i[PADDING_SYMBOL]):\n",
    "    source, target = zip(*batch)\n",
    "    max_source_len = max(map(len, source))\n",
    "    max_target_len = max(map(len, target))\n",
    "    padded_source = [[b[i] if i < len(b) else pad_source for i in range(max_source_len)] for b in source]\n",
    "    padded_target = [[l[i] if i < len(l) else pad_target for i in range(max_target_len)] for l in target]\n",
    "    return padded_source, padded_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(embedding_file):\n",
    "    \"\"\"\n",
    "    Reads pre-made embeddings from a file\n",
    "    \"\"\"\n",
    "    N = len(source_w2i)\n",
    "    embeddings = [0]*N\n",
    "    with codecs.open(embedding_file, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0].lower()\n",
    "            if word not in source_w2i:\n",
    "                source_w2i[word] = N\n",
    "                source_i2w.append(word)\n",
    "                N += 1\n",
    "                embeddings.append(0)\n",
    "            vec = [float(x) for x in data[1:]]\n",
    "            D = len(vec)\n",
    "            embeddings[source_w2i[word]] = vec\n",
    "    # Add a '0' embedding for the padding symbol\n",
    "    embeddings[0] = [0]*D\n",
    "    # Check if there are words that did not have a ready-made Glove embedding\n",
    "    # For these words, add a random vector\n",
    "    for word in source_w2i:\n",
    "        index = source_w2i[word]\n",
    "        if embeddings[index] == 0:\n",
    "            embeddings[index] = (np.random.random(D)-0.5).tolist()\n",
    "    return D, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Encoder ==================== #\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a batch of source sentences. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, no_of_input_symbols, embeddings=None, embedding_size=16, hidden_size=25,\n",
    "        encoder_bidirectional=False, device='cpu', use_gru=False, tune_embeddings=False) :\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.is_bidirectional = encoder_bidirectional\n",
    "        self.embedding = nn.Embedding(no_of_input_symbols,embedding_size)\n",
    "        if embeddings !=  None :\n",
    "            self.embedding.weight = nn.Parameter( torch.tensor(embeddings, dtype=torch.float), requires_grad=tune_embeddings )\n",
    "        if use_gru:\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True, bidirectional=self.is_bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=self.is_bidirectional)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def set_embeddings(self, embeddings):\n",
    "        self.embedding.weight = torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a list of lists of size (batch_size,max_seq_length)\n",
    "        Each inner list contains word IDs and represents one sentence.\n",
    "        The whole list-of-lists represents a batch of sentences.\n",
    "       \n",
    "        Returns:\n",
    "        the output from the encoder RNN: a pair of two tensors, one containing all hidden states, and one \n",
    "        containing the last hidden state (see https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "        \"\"\"\n",
    "\n",
    "        x_tensor = torch.tensor(x).to(self.device)\n",
    "        \n",
    "        # FOR TASK (a), REPLACE THE FOLLOWING LINE WITH YOUR CODE\n",
    "        embed_x = self.embedding(x_tensor)\n",
    "        return self.rnn(embed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Decoder ==================== #\n",
    "\n",
    "class DecoderRNN(nn.Module) :\n",
    "\n",
    "    def __init__(self, no_of_output_symbols, embedding_size=16, hidden_size=25, use_attention=True,\n",
    "        display_attention=False, device='cpu', use_gru=False) :\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(no_of_output_symbols,embedding_size)\n",
    "        self.no_of_output_symbols = no_of_output_symbols\n",
    "        self.W = nn.Parameter(torch.rand(hidden_size, hidden_size)-0.5) # shouldn't W be 2*hidden_size\n",
    "        self.U = nn.Parameter(torch.rand(hidden_size, hidden_size)-0.5)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size, 1)-0.5)\n",
    "        self.use_attention = use_attention\n",
    "        self.display_attention = display_attention\n",
    "        if use_gru:\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear( hidden_size, no_of_output_symbols )\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_outputs) :\n",
    "        \"\"\"\n",
    "        'input' is a list of length batch_size, containing the current word\n",
    "        of each sentence in the batch\n",
    "\n",
    "        'hidden' is a tensor containing the last hidden state of the decoder, \n",
    "        for each sequence in the batch\n",
    "        hidden.shape = (1, batch_size, hidden_size)\n",
    "\n",
    "        'encoder_outputs' is a tensor containing all hidden states from the\n",
    "        encoder (used in problem c)\n",
    "        encoder_outputs.shape = (batch_size, max_seq_length, hidden_size)\n",
    "\n",
    "        Note that 'max_seq_length' above refers to the max_seq_length\n",
    "        of the encoded sequence (not the decoded sequence).\n",
    "\n",
    "        Returns:\n",
    "        If use_attention and display_attention are both True (task (c)), return a triple\n",
    "        (logits for the predicted next word, hidden state, attention weights alpha)\n",
    "\n",
    "        Otherwise (task (b)), return a pair\n",
    "        (logits for the predicted next word, hidden state).\n",
    "        \"\"\"\n",
    "        inp_tensor = torch.tensor(inp).to(self.device)\n",
    "\n",
    "        # FOR (b) and (c) REPLACE THE FOLLOWING LINE WITH YOUR CODE\n",
    "        embed_inp = self.embedding(inp_tensor).unsqueeze(1) # (64 x 1 x 50)\n",
    "        if self.use_attention:\n",
    "            softmax = torch.nn.Softmax(dim=1)\n",
    "            first = encoder_outputs @ self.W\n",
    "            second = hidden.squeeze(0) @ self.U\n",
    "            summ = first + second.unsqueeze(1)\n",
    "            e = torch.tanh(summ) @ self.v\n",
    "            alphas = softmax(e)\n",
    "            context = torch.sum(alphas * encoder_outputs, dim=1).unsqueeze(0)\n",
    "            final, new_hidden = self.rnn(embed_inp, context)\n",
    "        else:\n",
    "            final, new_hidden = self.rnn(embed_inp, hidden)\n",
    "        if self.display_attention:\n",
    "            return self.output(final), new_hidden, alphas\n",
    "        else:\n",
    "            return self.output(final), new_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used for evaluation of both the dev set (during training)\n",
    "# and the test set (after training is finished).\n",
    "def evaluate(ds, encoder, decoder):\n",
    "    predicted_summaries = []\n",
    "    correct_summaries = []\n",
    "    for x, y in ds :\n",
    "        correct_summary = \"\"\n",
    "        predicted_summary = \"\"\n",
    "        outputs, hidden = encoder( [x] )\n",
    "        if encoder.is_bidirectional :\n",
    "            hidden = hidden.permute((1,0,2)).reshape(1,-1).unsqueeze(0)\n",
    "        predicted_symbol = target_w2i[START_SYMBOL]\n",
    "        for correct in y :\n",
    "            correct_summary +=  target_i2w[correct] + \" \"\n",
    "            predictions, hidden = decoder( [predicted_symbol], hidden, outputs )\n",
    "            _, predicted_tensor = predictions.topk(1)\n",
    "            predicted_symbol = predicted_tensor.detach().item()\n",
    "            predicted_summary +=  target_i2w[predicted_symbol] + \" \"\n",
    "        predicted_summaries.append(predicted_summary)\n",
    "        correct_summaries.append(correct_summary)\n",
    "    rouge = Rouge()\n",
    "    print(rouge.get_scores(predicted_summaries, correct_summaries, avg=True))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: NVIDIA H100 80GB HBM3 MIG 1g.10gb\n",
      "\n",
      "Number of source words:  412673\n",
      "Number of target words:  7262\n",
      "Number of training sentences:  20000\n",
      "\n",
      "19:15:27 Starting training.\n",
      "19:15:37 Epoch 0 loss: 14.669292449951172\n",
      "Evaluating on the dev data...\n",
      "{'rouge-1': {'r': 0.24542983288787787, 'p': 0.5589, 'f': 0.3261917213130668}, 'rouge-2': {'r': 0.003996666666666666, 'p': 0.0074, 'f': 0.004710649321581403}, 'rouge-l': {'r': 0.24542983288787787, 'p': 0.5589, 'f': 0.3261917213130668}}\n",
      "19:16:00 Epoch 1 loss: 9.700180053710938\n",
      "19:16:11 Epoch 2 loss: 9.252373695373535\n",
      "19:16:22 Epoch 3 loss: 8.94954776763916\n",
      "19:16:32 Epoch 4 loss: 8.71351432800293\n",
      "19:16:43 Epoch 5 loss: 8.515838623046875\n",
      "19:16:54 Epoch 6 loss: 8.341333389282227\n",
      "19:17:04 Epoch 7 loss: 8.186420440673828\n",
      "19:17:15 Epoch 8 loss: 8.047307968139648\n",
      "19:17:25 Epoch 9 loss: 7.919774055480957\n",
      "19:17:36 Epoch 10 loss: 7.799694538116455\n",
      "Evaluating on the dev data...\n",
      "{'rouge-1': {'r': 0.2311809388193151, 'p': 0.3496540476190371, 'f': 0.26899535574139544}, 'rouge-2': {'r': 0.028547296570531858, 'p': 0.04587500000000034, 'f': 0.033749915848607336}, 'rouge-l': {'r': 0.23107204993042618, 'p': 0.3494707142857038, 'f': 0.2688626817487214}}\n",
      "19:17:59 Epoch 11 loss: 7.683034420013428\n",
      "19:18:09 Epoch 12 loss: 7.567322254180908\n",
      "19:18:20 Epoch 13 loss: 7.450279235839844\n",
      "19:18:31 Epoch 14 loss: 7.331431865692139\n",
      "19:18:41 Epoch 15 loss: 7.214539051055908\n",
      "19:18:52 Epoch 16 loss: 7.114967346191406\n",
      "19:19:03 Epoch 17 loss: 7.030788421630859\n",
      "19:19:14 Epoch 18 loss: 6.940159797668457\n",
      "19:19:24 Epoch 19 loss: 6.836383819580078\n",
      "19:19:35 Epoch 20 loss: 6.7269439697265625\n",
      "Evaluating on the dev data...\n",
      "{'rouge-1': {'r': 0.20509041313726667, 'p': 0.29666881673881346, 'f': 0.23467059957106012}, 'rouge-2': {'r': 0.029189984079101724, 'p': 0.044219776334776514, 'f': 0.03379386265664528}, 'rouge-l': {'r': 0.20462617792803145, 'p': 0.29607191197690874, 'f': 0.23416232350278715}}\n",
      "19:19:58 Epoch 21 loss: 6.62968635559082\n",
      "19:20:08 Epoch 22 loss: 6.543752670288086\n",
      "19:20:18 Epoch 23 loss: 6.465425491333008\n",
      "19:20:29 Epoch 24 loss: 6.395390033721924\n",
      "19:20:39 Epoch 25 loss: 6.302794456481934\n",
      "19:20:49 Epoch 26 loss: 6.2111735343933105\n",
      "19:21:00 Epoch 27 loss: 6.134251117706299\n",
      "19:21:10 Epoch 28 loss: 6.064438819885254\n",
      "19:21:20 Epoch 29 loss: 5.991829872131348\n",
      "19:21:31 Epoch 30 loss: 5.922138690948486\n",
      "Evaluating on the dev data...\n",
      "{'rouge-1': {'r': 0.18126222583376783, 'p': 0.25646776445776565, 'f': 0.20532438045398216}, 'rouge-2': {'r': 0.025324828031445697, 'p': 0.03758509379509388, 'f': 0.029136518773031434}, 'rouge-l': {'r': 0.18067992202646396, 'p': 0.2557669830169843, 'f': 0.20470321758224141}}\n",
      "19:21:52 Epoch 31 loss: 5.849353313446045\n",
      "19:22:03 Epoch 32 loss: 5.772157192230225\n",
      "19:22:13 Epoch 33 loss: 5.708383560180664\n",
      "19:22:23 Epoch 34 loss: 5.645762920379639\n",
      "19:22:34 Epoch 35 loss: 5.587161540985107\n",
      "19:22:44 Epoch 36 loss: 5.536124229431152\n",
      "19:22:54 Epoch 37 loss: 5.466421604156494\n",
      "19:23:05 Epoch 38 loss: 5.400698661804199\n",
      "19:23:15 Epoch 39 loss: 5.342656135559082\n",
      "19:23:25 Epoch 40 loss: 5.305018901824951\n",
      "Evaluating on the dev data...\n",
      "{'rouge-1': {'r': 0.17190107003570657, 'p': 0.24416269341769467, 'f': 0.19486743900431258}, 'rouge-2': {'r': 0.02490952675864444, 'p': 0.03748573593073596, 'f': 0.02881924354883045}, 'rouge-l': {'r': 0.17134613996577647, 'p': 0.24348641636141782, 'f': 0.19426931355992552}}\n",
      "19:23:47 Epoch 41 loss: 5.250936508178711\n",
      "19:23:57 Epoch 42 loss: 5.200589656829834\n",
      "19:24:08 Epoch 43 loss: 5.1620073318481445\n",
      "19:24:18 Epoch 44 loss: 5.117491722106934\n",
      "19:24:28 Epoch 45 loss: 5.059920787811279\n",
      "19:24:39 Epoch 46 loss: 4.998000144958496\n",
      "19:24:49 Epoch 47 loss: 4.953378200531006\n",
      "19:24:59 Epoch 48 loss: 4.900882720947266\n",
      "19:25:10 Epoch 49 loss: 4.854867935180664\n",
      "Evaluating on the test data...\n",
      "Number of test sentences:  5000\n",
      "\n",
      "{'rouge-1': {'r': 0.1606933265743178, 'p': 0.22576045454545637, 'f': 0.1814935338992728}, 'rouge-2': {'r': 0.02167122247034017, 'p': 0.03223494227994228, 'f': 0.024864504347324942}, 'rouge-l': {'r': 0.15981474082999678, 'p': 0.22468702020202208, 'f': 0.18054851615009745}}\n"
     ]
    }
   ],
   "source": [
    "# Use 'Run all cells' to do the training.\n",
    "\n",
    "# ================ Hyper-parameters ================ #\n",
    "\n",
    "use_attention = True     \n",
    "use_gru = True         # Use Gated Recurrent Units (rather than plain RNNs)\n",
    "bidirectional = True   # Use a bidirectional encoder\n",
    "use_embeddings = True      # Use pre-loaded Glove embeddings\n",
    "tune_embeddings = True # Fine-tune the Glove embeddings\n",
    "batch_size = 64\n",
    "hidden_size = 25       # Number of dimensions in the hidden state\n",
    "learning_rate = 0.001\n",
    "epochs = 50            # We will train for this many epochs\n",
    "save = False           # Do not save the model\n",
    "\n",
    "# ==================== Training ==================== #\n",
    "# Reproducibility\n",
    "# Read a bit more here -- https://pytorch.org/docs/stable/notes/randomness.html\n",
    "# random.seed(5719)\n",
    "# np.random.seed(5719)\n",
    "#torch.manual_seed(5719)\n",
    "#torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Can we run on GPU?\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print('Running on CPU')\n",
    "print()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "print( \"Number of source words: \", len(source_i2w) )\n",
    "print( \"Number of target words: \", len(target_i2w) )\n",
    "print( \"Number of training sentences: \", len(training_dataset) )\n",
    "print()\n",
    "\n",
    "# If we have pre-computed word embeddings, then make sure these are used\n",
    "if use_embeddings:\n",
    "    embedding_size, embeddings = load_glove_embeddings('/datasets/dd2417/glove.6B.50d.txt')\n",
    "else :\n",
    "    embedding_size = args.hidden_size\n",
    "    embeddings = None\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, collate_fn=pad_sequence)\n",
    "dev_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=pad_sequence)\n",
    "        \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "    len(source_i2w),\n",
    "    embeddings=embeddings,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    encoder_bidirectional=bidirectional,\n",
    "    tune_embeddings=tune_embeddings,\n",
    "    use_gru=use_gru,\n",
    "    device=device\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    len(target_i2w),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size*(bidirectional+1),\n",
    "    use_attention=use_attention,\n",
    "    use_gru=use_gru,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "print( datetime.now().strftime(\"%H:%M:%S\"), \"Starting training.\" )\n",
    "\n",
    "for epoch in range( epochs ) :\n",
    "    total_loss = 0\n",
    "    for source, target in training_loader: #tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        # hidden is (D * num_layers, B, H)\n",
    "        outputs, hidden = encoder( source )\n",
    "        if bidirectional:\n",
    "            # (2, B, H) -> (B, 2 * H) -> (1, B, 2 * H)\n",
    "            hidden = torch.cat([hidden[0,:, :], hidden[1,:,:]], dim=1).unsqueeze(0)\n",
    "                    \n",
    "        # The probability of doing teacher forcing will decrease\n",
    "        # from 1 to 0 over the range of epochs. This could be implemented\n",
    "        # like this:\n",
    "        # teacher_forcing_ratio = 1- epoch/args.epochs\n",
    "        # But, for now we will always use teacher forcing\n",
    "        teacher_forcing_ratio = 1\n",
    "\n",
    "        # The input to the decoder in the first time step will be\n",
    "        # the boundary symbol, regardless if we are using teacher\n",
    "        # forcing or not.\n",
    "        idx = [target_w2i[START_SYMBOL] for sublist in target]\n",
    "        predicted_symbol = [target_w2i[START_SYMBOL] for sublist in target]\n",
    "\n",
    "        target_length = len(target[0])\n",
    "        for i in range(target_length) :\n",
    "            use_teacher_forcing = (random.random() < teacher_forcing_ratio)\n",
    "            if use_teacher_forcing :\n",
    "                predictions, hidden = decoder( idx, hidden, outputs )\n",
    "            else:\n",
    "                # Here we input the previous prediction rather than the\n",
    "                # correct symbol.\n",
    "                predictions, hidden = decoder( predicted_symbol, hidden, outputs )\n",
    "            _, predicted_tensor = predictions.topk(1)\n",
    "            predicted_symbol = predicted_tensor.squeeze().tolist()\n",
    "\n",
    "            # The targets will be the ith symbol of all the target\n",
    "            # strings. They will also be used as inputs for the next\n",
    "            # time step if we use teacher forcing.\n",
    "            idx = [sublist[i] for sublist in target]\n",
    "            loss += criterion( predictions.squeeze(), torch.tensor(idx).to(device) )\n",
    "        loss /= (target_length * batch_size)\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss\n",
    "    print( datetime.now().strftime(\"%H:%M:%S\"), \"Epoch\", epoch, \"loss:\", total_loss.detach().item() )\n",
    "    total_loss = 0\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Evaluating on the dev data...\")\n",
    "        evaluate(test_dataset, encoder, decoder)\n",
    "\n",
    "# ==================== Save the model  ==================== #\n",
    "\n",
    "if ( save ) :\n",
    "    dt = str(datetime.now()).replace(' ','_').replace(':','_').replace('.','_')\n",
    "    newdir = 'model_' + dt\n",
    "    os.mkdir( newdir )\n",
    "    torch.save( encoder.state_dict(), os.path.join(newdir, 'encoder.model') )\n",
    "    torch.save( decoder.state_dict(), os.path.join(newdir, 'decoder.model') )\n",
    "    with open( os.path.join(newdir, 'source_w2i'), 'wb' ) as f :\n",
    "        pickle.dump( source_w2i, f )\n",
    "        f.close()\n",
    "    with open( os.path.join(newdir, 'source_i2w'), 'wb' ) as f :\n",
    "        pickle.dump( source_i2w, f )\n",
    "        f.close()\n",
    "    with open( os.path.join(newdir, 'target_w2i'), 'wb' ) as f :\n",
    "        pickle.dump( target_w2i, f )\n",
    "        f.close()\n",
    "    with open( os.path.join(newdir, 'target_i2w'), 'wb' ) as f :\n",
    "        pickle.dump( target_i2w, f )\n",
    "        f.close()\n",
    "\n",
    "    settings = {\n",
    "        'training_set': training_file,\n",
    "        'test_set': test_file,\n",
    "        'epochs': epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'attention': attention,\n",
    "        'bidirectional': bidirectional,\n",
    "        'embedding_size': embedding_size,\n",
    "        'use_gru': use_gru,\n",
    "        'tune_embeddings': tune_embeddings\n",
    "    }\n",
    "    with open( os.path.join(newdir, 'settings.json'), 'w' ) as f:\n",
    "        json.dump(settings, f)\n",
    "\n",
    "# ==================== Evaluation ==================== #\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "print( \"Evaluating on the test data...\" )\n",
    "\n",
    "print( \"Number of test sentences: \", len(test_dataset) )\n",
    "print()\n",
    "\n",
    "evaluate(test_dataset, encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great product , but pricey as the market <END> \n",
      "+---------------+-------+---------+------+------+--------+------+------+--------+-------+\n",
      "| Source/Result | great | product | ,    | but  | pricey | as   | the  | market | <END> |\n",
      "+---------------+-------+---------+------+------+--------+------+------+--------+-------+\n",
      "| bought        | 0.01  | 0.00    | 0.00 | 0.00 | 0.00   | 0.00 | 0.00 | 0.00   | 0.00  |\n",
      "| several       | 0.05  | 0.04    | 0.00 | 0.00 | 0.03   | 0.01 | 0.00 | 0.00   | 0.00  |\n",
      "| vitality      | 0.02  | 0.01    | 0.17 | 0.00 | 0.02   | 0.00 | 0.00 | 0.05   | 0.00  |\n",
      "| canned        | 0.01  | 0.00    | 0.05 | 0.00 | 0.01   | 0.01 | 0.00 | 0.00   | 0.00  |\n",
      "| dog           | 0.00  | 0.16    | 0.00 | 0.00 | 0.00   | 0.00 | 0.00 | 0.00   | 0.00  |\n",
      "| food          | 0.00  | 0.00    | 0.01 | 0.00 | 0.00   | 0.01 | 0.00 | 0.00   | 0.00  |\n",
      "| products      | 0.00  | 0.02    | 0.01 | 0.00 | 0.00   | 0.02 | 0.00 | 0.00   | 0.00  |\n",
      "| found         | 0.01  | 0.00    | 0.01 | 0.00 | 0.00   | 0.00 | 0.00 | 0.00   | 0.00  |\n",
      "| good          | 0.22  | 0.00    | 0.11 | 0.95 | 0.03   | 0.02 | 0.87 | 0.55   | 0.95  |\n",
      "| quality       | 0.03  | 0.00    | 0.00 | 0.00 | 0.00   | 0.01 | 0.02 | 0.00   | 0.00  |\n",
      "| product       | 0.01  | 0.21    | 0.01 | 0.01 | 0.00   | 0.00 | 0.00 | 0.00   | 0.00  |\n",
      "| looks         | 0.08  | 0.00    | 0.12 | 0.00 | 0.00   | 0.02 | 0.00 | 0.00   | 0.00  |\n",
      "| like          | 0.01  | 0.00    | 0.03 | 0.00 | 0.00   | 0.01 | 0.00 | 0.00   | 0.00  |\n",
      "| stew          | 0.00  | 0.42    | 0.00 | 0.00 | 0.00   | 0.00 | 0.00 | 0.00   | 0.00  |\n",
      "| processed     | 0.01  | 0.00    | 0.03 | 0.00 | 0.01   | 0.01 | 0.00 | 0.00   | 0.00  |\n",
      "| meat          | 0.01  | 0.06    | 0.01 | 0.00 | 0.00   | 0.03 | 0.00 | 0.00   | 0.00  |\n",
      "| smells        | 0.03  | 0.00    | 0.13 | 0.00 | 0.01   | 0.01 | 0.00 | 0.01   | 0.00  |\n",
      "| better        | 0.07  | 0.00    | 0.10 | 0.03 | 0.00   | 0.01 | 0.11 | 0.09   | 0.04  |\n",
      "| labrador      | 0.00  | 0.00    | 0.00 | 0.00 | 0.00   | 0.00 | 0.00 | 0.00   | 0.00  |\n",
      "| finicky       | 0.00  | 0.00    | 0.00 | 0.00 | 0.00   | 0.00 | 0.00 | 0.00   | 0.00  |\n",
      "| appreciates   | 0.06  | 0.05    | 0.11 | 0.00 | 0.85   | 0.82 | 0.00 | 0.20   | 0.00  |\n",
      "| product       | 0.34  | 0.02    | 0.00 | 0.00 | 0.01   | 0.00 | 0.00 | 0.00   | 0.00  |\n",
      "| better        | 0.04  | 0.00    | 0.11 | 0.01 | 0.00   | 0.00 | 0.00 | 0.09   | 0.01  |\n",
      "| .             | 0.00  | 0.00    | 0.00 | 0.00 | 0.00   | 0.00 | 0.00 | 0.00   | 0.00  |\n",
      "+---------------+-------+---------+------+------+--------+------+------+--------+-------+\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not what i love the best thing since sliced a lot of the best thing since sliced a lot of \n",
      "+---------------+------+------+------+------+------+------+-------+-------+--------+------+------+------+------+------+-------+-------+--------+------+------+------+\n",
      "| Source/Result | not  | what | i    | love | the  | best | thing | since | sliced | a    | lot  | of   | the  | best | thing | since | sliced | a    | lot  | of   |\n",
      "+---------------+------+------+------+------+------+------+-------+-------+--------+------+------+------+------+------+-------+-------+--------+------+------+------+\n",
      "| product       | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.00 | 0.00  | 0.01  | 0.00   | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.02  | 0.00   | 0.00 | 0.00 | 0.00 |\n",
      "| arrived       | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 |\n",
      "| labeled       | 0.00 | 0.02 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.00  | 0.01  | 0.00   | 0.00 | 0.00 | 0.00 |\n",
      "| jumbo         | 0.01 | 0.04 | 0.11 | 0.13 | 0.01 | 0.37 | 0.00  | 0.01  | 0.38   | 0.08 | 0.03 | 0.00 | 0.29 | 0.01 | 0.00  | 0.01  | 0.40   | 0.08 | 0.03 | 0.00 |\n",
      "| salted        | 0.00 | 0.00 | 0.00 | 0.01 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 |\n",
      "| peanuts       | 0.01 | 0.26 | 0.05 | 0.04 | 0.00 | 0.03 | 0.00  | 0.04  | 0.02   | 0.06 | 0.02 | 0.00 | 0.01 | 0.02 | 0.00  | 0.05  | 0.02   | 0.06 | 0.02 | 0.00 |\n",
      "| peanuts       | 0.01 | 0.28 | 0.08 | 0.05 | 0.00 | 0.06 | 0.00  | 0.04  | 0.02   | 0.05 | 0.02 | 0.00 | 0.01 | 0.03 | 0.00  | 0.05  | 0.02   | 0.05 | 0.02 | 0.00 |\n",
      "| actually      | 0.05 | 0.00 | 0.38 | 0.03 | 0.04 | 0.01 | 0.01  | 0.00  | 0.06   | 0.07 | 0.17 | 0.01 | 0.01 | 0.33 | 0.01  | 0.01  | 0.06   | 0.08 | 0.17 | 0.01 |\n",
      "| small         | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 |\n",
      "| sized         | 0.02 | 0.02 | 0.03 | 0.00 | 0.00 | 0.02 | 0.00  | 0.01  | 0.02   | 0.17 | 0.01 | 0.01 | 0.25 | 0.01 | 0.00  | 0.02  | 0.02   | 0.16 | 0.01 | 0.01 |\n",
      "| unsalted      | 0.00 | 0.00 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.01 | 0.00 |\n",
      "| sure          | 0.00 | 0.00 | 0.15 | 0.01 | 0.00 | 0.19 | 0.00  | 0.00  | 0.02   | 0.01 | 0.33 | 0.00 | 0.03 | 0.18 | 0.00  | 0.00  | 0.02   | 0.01 | 0.33 | 0.00 |\n",
      "| error         | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 |\n",
      "| vendor        | 0.01 | 0.08 | 0.02 | 0.01 | 0.00 | 0.00 | 0.82  | 0.02  | 0.01   | 0.04 | 0.00 | 0.05 | 0.02 | 0.01 | 0.64  | 0.05  | 0.01   | 0.04 | 0.00 | 0.05 |\n",
      "| intended      | 0.74 | 0.07 | 0.03 | 0.01 | 0.79 | 0.01 | 0.00  | 0.71  | 0.42   | 0.38 | 0.10 | 0.66 | 0.21 | 0.02 | 0.01  | 0.56  | 0.39   | 0.37 | 0.10 | 0.65 |\n",
      "| represent     | 0.14 | 0.19 | 0.09 | 0.63 | 0.14 | 0.09 | 0.15  | 0.12  | 0.03   | 0.12 | 0.28 | 0.26 | 0.12 | 0.29 | 0.33  | 0.16  | 0.03   | 0.13 | 0.28 | 0.26 |\n",
      "| product       | 0.00 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.03  | 0.00   | 0.00 | 0.00 | 0.00 | 0.00 | 0.04 | 0.01  | 0.03  | 0.00   | 0.00 | 0.00 | 0.00 |\n",
      "| jumbo         | 0.00 | 0.03 | 0.04 | 0.07 | 0.01 | 0.22 | 0.00  | 0.00  | 0.02   | 0.01 | 0.01 | 0.00 | 0.04 | 0.05 | 0.00  | 0.00  | 0.02   | 0.01 | 0.01 | 0.00 |\n",
      "| .             | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00  | 0.00   | 0.00 | 0.00 | 0.00 |\n",
      "+---------------+------+------+------+------+------+------+-------+-------+--------+------+------+------+------+------+-------+-------+--------+------+------+------+\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my dog loves these ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! \n",
      "+---------------+------+------+-------+-------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "| Source/Result | my   | dog  | loves | these | !    | !    | !    | !    | !    | !    | !    | !    | !    | !    | !    | !    | !    | !    | !    | !    |\n",
      "+---------------+------+------+-------+-------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "| confection    | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| around        | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| centuries     | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| light         | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| pillowy       | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| citrus        | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| gelatin       | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| nuts          | 0.00 | 0.00 | 0.00  | 0.25  | 0.00 | 0.00 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |\n",
      "| case          | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| filberts      | 0.22 | 0.00 | 0.10  | 0.22  | 0.00 | 0.00 | 0.06 | 0.02 | 0.03 | 0.02 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 |\n",
      "| cut           | 0.00 | 0.00 | 0.00  | 0.01  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| tiny          | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| squares       | 0.00 | 0.09 | 0.00  | 0.01  | 0.00 | 0.00 | 0.03 | 0.01 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |\n",
      "| liberally     | 0.00 | 0.02 | 0.00  | 0.05  | 0.00 | 0.00 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |\n",
      "| coated        | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| powdered      | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| sugar         | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| tiny          | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| mouthful      | 0.00 | 0.00 | 0.01  | 0.01  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| heaven        | 0.27 | 0.07 | 0.09  | 0.01  | 0.00 | 0.00 | 0.14 | 0.06 | 0.09 | 0.07 | 0.08 | 0.08 | 0.08 | 0.08 | 0.08 | 0.08 | 0.08 | 0.08 | 0.08 | 0.08 |\n",
      "| chewy         | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| flavorful     | 0.00 | 0.00 | 0.00  | 0.04  | 0.56 | 0.00 | 0.01 | 0.02 | 0.01 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |\n",
      "| highly        | 0.00 | 0.01 | 0.00  | 0.01  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| recommend     | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| yummy         | 0.00 | 0.00 | 0.00  | 0.03  | 0.20 | 0.06 | 0.00 | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |\n",
      "| treat         | 0.01 | 0.01 | 0.01  | 0.01  | 0.00 | 0.00 | 0.20 | 0.23 | 0.23 | 0.23 | 0.23 | 0.23 | 0.23 | 0.23 | 0.23 | 0.23 | 0.23 | 0.23 | 0.23 | 0.23 |\n",
      "| familiar      | 0.00 | 0.00 | 0.01  | 0.01  | 0.00 | 0.01 | 0.02 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 |\n",
      "| story         | 0.00 | 0.00 | 0.00  | 0.12  | 0.24 | 0.93 | 0.02 | 0.04 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 |\n",
      "| c             | 0.00 | 0.01 | 0.00  | 0.01  | 0.00 | 0.00 | 0.08 | 0.05 | 0.07 | 0.06 | 0.07 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 |\n",
      "| lewis         | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| lion          | 0.12 | 0.29 | 0.03  | 0.00  | 0.00 | 0.00 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |\n",
      "| witch         | 0.02 | 0.45 | 0.02  | 0.02  | 0.00 | 0.00 | 0.09 | 0.13 | 0.13 | 0.13 | 0.13 | 0.13 | 0.13 | 0.13 | 0.13 | 0.13 | 0.13 | 0.13 | 0.13 | 0.13 |\n",
      "| wardrobe      | 0.00 | 0.01 | 0.02  | 0.00  | 0.00 | 0.00 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |\n",
      "| treat         | 0.00 | 0.02 | 0.06  | 0.02  | 0.00 | 0.00 | 0.15 | 0.20 | 0.18 | 0.19 | 0.19 | 0.19 | 0.19 | 0.19 | 0.19 | 0.19 | 0.19 | 0.19 | 0.19 | 0.19 |\n",
      "| seduces       | 0.32 | 0.00 | 0.63  | 0.08  | 0.00 | 0.00 | 0.05 | 0.08 | 0.06 | 0.07 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 | 0.06 |\n",
      "| edmund        | 0.03 | 0.01 | 0.01  | 0.05  | 0.00 | 0.00 | 0.04 | 0.04 | 0.04 | 0.04 | 0.04 | 0.04 | 0.04 | 0.04 | 0.04 | 0.04 | 0.04 | 0.04 | 0.04 | 0.04 |\n",
      "| selling       | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| brother       | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| sisters       | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
      "| witch         | 0.00 | 0.00 | 0.00  | 0.00  | 0.00 | 0.00 | 0.00 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |\n",
      "+---------------+------+------+-------+-------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great product , but not a great product , but not a great product , but not a great product \n",
      "+---------------+-------+---------+------+------+------+------+-------+---------+------+------+------+------+-------+---------+------+------+------+------+-------+---------+\n",
      "| Source/Result | great | product | ,    | but  | not  | a    | great | product | ,    | but  | not  | a    | great | product | ,    | but  | not  | a    | great | product |\n",
      "+---------------+-------+---------+------+------+------+------+-------+---------+------+------+------+------+-------+---------+------+------+------+------+-------+---------+\n",
      "| confection    | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| around        | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| centuries     | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| light         | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| pillowy       | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| citrus        | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| gelatin       | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| nuts          | 0.01  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.01  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.01  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.01  | 0.00    |\n",
      "| case          | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| filberts      | 0.04  | 0.00    | 0.01 | 0.01 | 0.00 | 0.00 | 0.30  | 0.12    | 0.00 | 0.00 | 0.00 | 0.00 | 0.30  | 0.12    | 0.00 | 0.00 | 0.00 | 0.00 | 0.30  | 0.12    |\n",
      "| cut           | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    |\n",
      "| tiny          | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| squares       | 0.00  | 0.01    | 0.00 | 0.00 | 0.14 | 0.18 | 0.00  | 0.20    | 0.00 | 0.00 | 0.13 | 0.18 | 0.00  | 0.20    | 0.00 | 0.00 | 0.13 | 0.18 | 0.00  | 0.20    |\n",
      "| liberally     | 0.03  | 0.14    | 0.00 | 0.00 | 0.15 | 0.06 | 0.00  | 0.06    | 0.00 | 0.00 | 0.16 | 0.07 | 0.00  | 0.06    | 0.00 | 0.00 | 0.16 | 0.07 | 0.00  | 0.06    |\n",
      "| coated        | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| powdered      | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| sugar         | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| tiny          | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| mouthful      | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.01    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.01    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.01    |\n",
      "| heaven        | 0.05  | 0.00    | 0.00 | 0.00 | 0.01 | 0.01 | 0.00  | 0.18    | 0.00 | 0.00 | 0.01 | 0.01 | 0.00  | 0.18    | 0.00 | 0.00 | 0.01 | 0.01 | 0.00  | 0.18    |\n",
      "| chewy         | 0.03  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| flavorful     | 0.02  | 0.00    | 0.00 | 0.07 | 0.02 | 0.03 | 0.01  | 0.00    | 0.00 | 0.08 | 0.02 | 0.03 | 0.01  | 0.00    | 0.00 | 0.08 | 0.02 | 0.03 | 0.01  | 0.00    |\n",
      "| highly        | 0.00  | 0.06    | 0.00 | 0.00 | 0.09 | 0.07 | 0.00  | 0.02    | 0.00 | 0.00 | 0.09 | 0.07 | 0.00  | 0.02    | 0.00 | 0.00 | 0.09 | 0.07 | 0.00  | 0.02    |\n",
      "| recommend     | 0.09  | 0.01    | 0.01 | 0.00 | 0.01 | 0.04 | 0.05  | 0.00    | 0.01 | 0.00 | 0.01 | 0.04 | 0.05  | 0.00    | 0.01 | 0.00 | 0.01 | 0.04 | 0.05  | 0.00    |\n",
      "| yummy         | 0.00  | 0.00    | 0.01 | 0.08 | 0.01 | 0.03 | 0.02  | 0.00    | 0.00 | 0.10 | 0.01 | 0.03 | 0.02  | 0.00    | 0.00 | 0.10 | 0.01 | 0.03 | 0.02  | 0.00    |\n",
      "| treat         | 0.05  | 0.01    | 0.36 | 0.07 | 0.03 | 0.02 | 0.03  | 0.02    | 0.46 | 0.06 | 0.04 | 0.02 | 0.03  | 0.02    | 0.46 | 0.06 | 0.04 | 0.02 | 0.03  | 0.02    |\n",
      "| familiar      | 0.42  | 0.00    | 0.01 | 0.20 | 0.01 | 0.02 | 0.23  | 0.02    | 0.01 | 0.20 | 0.01 | 0.02 | 0.23  | 0.02    | 0.01 | 0.20 | 0.01 | 0.02 | 0.23  | 0.02    |\n",
      "| story         | 0.05  | 0.00    | 0.21 | 0.50 | 0.01 | 0.20 | 0.23  | 0.02    | 0.22 | 0.50 | 0.02 | 0.20 | 0.23  | 0.02    | 0.22 | 0.50 | 0.02 | 0.20 | 0.23  | 0.02    |\n",
      "| c             | 0.01  | 0.00    | 0.01 | 0.02 | 0.01 | 0.01 | 0.00  | 0.02    | 0.01 | 0.01 | 0.01 | 0.01 | 0.00  | 0.02    | 0.01 | 0.01 | 0.01 | 0.01 | 0.00  | 0.02    |\n",
      "| lewis         | 0.02  | 0.00    | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    | 0.01 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    | 0.01 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    |\n",
      "| lion          | 0.02  | 0.24    | 0.00 | 0.00 | 0.02 | 0.01 | 0.01  | 0.12    | 0.00 | 0.00 | 0.02 | 0.01 | 0.01  | 0.12    | 0.00 | 0.00 | 0.02 | 0.01 | 0.01  | 0.12    |\n",
      "| witch         | 0.01  | 0.44    | 0.00 | 0.00 | 0.04 | 0.02 | 0.00  | 0.05    | 0.00 | 0.00 | 0.05 | 0.02 | 0.00  | 0.05    | 0.00 | 0.00 | 0.05 | 0.02 | 0.00  | 0.05    |\n",
      "| wardrobe      | 0.00  | 0.08    | 0.00 | 0.00 | 0.02 | 0.07 | 0.03  | 0.04    | 0.00 | 0.00 | 0.02 | 0.07 | 0.03  | 0.04    | 0.00 | 0.00 | 0.02 | 0.07 | 0.03  | 0.04    |\n",
      "| treat         | 0.03  | 0.00    | 0.36 | 0.03 | 0.01 | 0.01 | 0.01  | 0.03    | 0.24 | 0.03 | 0.01 | 0.01 | 0.01  | 0.03    | 0.24 | 0.03 | 0.01 | 0.01 | 0.01  | 0.03    |\n",
      "| seduces       | 0.01  | 0.00    | 0.01 | 0.00 | 0.39 | 0.08 | 0.02  | 0.07    | 0.01 | 0.00 | 0.37 | 0.07 | 0.02  | 0.07    | 0.01 | 0.00 | 0.37 | 0.07 | 0.02  | 0.07    |\n",
      "| edmund        | 0.09  | 0.00    | 0.00 | 0.00 | 0.03 | 0.09 | 0.03  | 0.01    | 0.01 | 0.00 | 0.03 | 0.09 | 0.03  | 0.01    | 0.01 | 0.00 | 0.03 | 0.09 | 0.03  | 0.01    |\n",
      "| selling       | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| brother       | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    |\n",
      "| sisters       | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "| witch         | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.01 | 0.00  | 0.00    |\n",
      "| .             | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00  | 0.00    |\n",
      "+---------------+-------+---------+------+------+------+------+-------+---------+------+------+------+------+-------+---------+------+------+------+------+-------+---------+\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great product . <END> \n",
      "+---------------+-------+---------+------+-------+\n",
      "| Source/Result | great | product | .    | <END> |\n",
      "+---------------+-------+---------+------+-------+\n",
      "| looking       | 0.00  | 0.00    | 0.00 | 0.00  |\n",
      "| secret        | 0.00  | 0.31    | 0.00 | 0.00  |\n",
      "| ingredient    | 0.06  | 0.03    | 0.00 | 0.48  |\n",
      "| robitussin    | 0.00  | 0.63    | 0.00 | 0.00  |\n",
      "| believe       | 0.01  | 0.00    | 0.00 | 0.00  |\n",
      "| found         | 0.00  | 0.00    | 0.00 | 0.00  |\n",
      "| got           | 0.28  | 0.00    | 0.00 | 0.03  |\n",
      "| addition      | 0.00  | 0.00    | 0.00 | 0.00  |\n",
      "| root          | 0.12  | 0.03    | 0.97 | 0.21  |\n",
      "| beer          | 0.01  | 0.00    | 0.00 | 0.00  |\n",
      "| extract       | 0.00  | 0.01    | 0.00 | 0.00  |\n",
      "| ordered       | 0.00  | 0.00    | 0.00 | 0.00  |\n",
      "| good          | 0.31  | 0.00    | 0.01 | 0.27  |\n",
      "| made          | 0.01  | 0.00    | 0.00 | 0.00  |\n",
      "| cherry        | 0.07  | 0.00    | 0.01 | 0.01  |\n",
      "| soda          | 0.00  | 0.00    | 0.00 | 0.00  |\n",
      "| flavor        | 0.12  | 0.00    | 0.00 | 0.00  |\n",
      "| medicinal     | 0.00  | 0.00    | 0.00 | 0.00  |\n",
      "| .             | 0.00  | 0.00    | 0.00 | 0.00  |\n",
      "+---------------+-------+---------+------+-------+\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  great taffy great price wide assortment yummy taffy delivery quick taffy lover deal.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great service <END> \n",
      "+---------------+-------+---------+-------+\n",
      "| Source/Result | great | service | <END> |\n",
      "+---------------+-------+---------+-------+\n",
      "| great         | 0.00  | 0.00    | 0.00  |\n",
      "| taffy         | 0.00  | 0.00    | 0.00  |\n",
      "| great         | 0.01  | 0.01    | 0.01  |\n",
      "| price         | 0.00  | 0.00    | 0.00  |\n",
      "| wide          | 0.00  | 0.00    | 0.00  |\n",
      "| assortment    | 0.00  | 0.02    | 0.00  |\n",
      "| yummy         | 0.00  | 0.00    | 0.00  |\n",
      "| taffy         | 0.00  | 0.00    | 0.00  |\n",
      "| delivery      | 0.00  | 0.00    | 0.00  |\n",
      "| quick         | 0.00  | 0.00    | 0.00  |\n",
      "| taffy         | 0.00  | 0.00    | 0.00  |\n",
      "| lover         | 0.99  | 0.97    | 0.99  |\n",
      "| deal          | 0.00  | 0.00    | 0.00  |\n",
      "| .             | 0.00  | 0.00    | 0.00  |\n",
      "+---------------+-------+---------+-------+\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  great dog food product.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great product <END> \n",
      "+---------------+-------+---------+-------+\n",
      "| Source/Result | great | product | <END> |\n",
      "+---------------+-------+---------+-------+\n",
      "| great         | 0.98  | 0.22    | 0.78  |\n",
      "| dog           | 0.01  | 0.74    | 0.00  |\n",
      "| food          | 0.02  | 0.01    | 0.22  |\n",
      "| product       | 0.00  | 0.02    | 0.00  |\n",
      "| .             | 0.00  | 0.00    | 0.00  |\n",
      "+---------------+-------+---------+-------+\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  great dog food product for a cheap price.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great product <END> \n",
      "+---------------+-------+---------+-------+\n",
      "| Source/Result | great | product | <END> |\n",
      "+---------------+-------+---------+-------+\n",
      "| great         | 0.92  | 0.04    | 0.46  |\n",
      "| dog           | 0.00  | 0.56    | 0.01  |\n",
      "| food          | 0.01  | 0.01    | 0.05  |\n",
      "| product       | 0.02  | 0.34    | 0.01  |\n",
      "| for           | 0.01  | 0.01    | 0.09  |\n",
      "| a             | 0.00  | 0.01    | 0.03  |\n",
      "| cheap         | 0.04  | 0.03    | 0.31  |\n",
      "| price         | 0.00  | 0.01    | 0.04  |\n",
      "| .             | 0.00  | 0.00    | 0.00  |\n",
      "+---------------+-------+---------+-------+\n"
     ]
    }
   ],
   "source": [
    "# ==================== User interaction ==================== #\n",
    "\n",
    "decoder.display_attention = True\n",
    "while( True ) :\n",
    "    text = input( \"> \" )\n",
    "    if text == \"\" :\n",
    "        continue\n",
    "    try :\n",
    "        source_sentence = [source_w2i[w] for w in nltk.word_tokenize(text)]\n",
    "    except KeyError :\n",
    "        print( \"Erroneous input string\" )\n",
    "        continue\n",
    "    outputs, hidden = encoder( [source_sentence] )\n",
    "    if encoder.is_bidirectional :\n",
    "        hidden = hidden.permute((1,0,2)).reshape(1,-1).unsqueeze(0)\n",
    "        \n",
    "    predicted_symbol = target_w2i[START_SYMBOL]\n",
    "    target_sentence = []\n",
    "    attention_probs = []    \n",
    "    num_attempts = 0\n",
    "    while num_attempts < MAX_PREDICTIONS:\n",
    "        if use_attention :\n",
    "            predictions, hidden, alpha = decoder( [predicted_symbol], hidden, outputs )\n",
    "            attention_probs.append( alpha.permute(0,2,1).squeeze().detach().tolist() )\n",
    "        else :\n",
    "            predictions, hidden = decoder( [predicted_symbol], hidden, outputs )\n",
    "            \n",
    "        _, predicted_tensor = predictions.topk(1)\n",
    "        predicted_symbol = predicted_tensor.detach().item()\n",
    "        target_sentence.append( predicted_symbol )\n",
    "\n",
    "        num_attempts += 1\n",
    "\n",
    "        if predicted_symbol == target_w2i[END_SYMBOL] :\n",
    "            break\n",
    "\n",
    "    for i in target_sentence :\n",
    "        print( target_i2w[i].encode('utf-8').decode(), end=' ' )\n",
    "    print()\n",
    "\n",
    "    if use_attention :\n",
    "        # Construct the attention table\n",
    "        ap = torch.tensor(attention_probs).T\n",
    "        if len(ap.shape) == 1:\n",
    "            ap = ap.unsqueeze(0)\n",
    "        attention_probs = ap.tolist()\n",
    "            \n",
    "        for i in range(len(attention_probs)) :\n",
    "            for j in range(len(attention_probs[i])) :\n",
    "                attention_probs[i][j] = \"{val:.2f}\".format(val=attention_probs[i][j])\n",
    "        for i in range(len(attention_probs)) :\n",
    "            if i<len(text) :\n",
    "                attention_probs[i].insert(0,source_i2w[source_sentence[i]])\n",
    "            else :\n",
    "                attention_probs[i].insert(0,' ')\n",
    "        first_row = [\"Source/Result\"]\n",
    "        for w in target_sentence :\n",
    "            first_row.append(target_i2w[w])\n",
    "        attention_probs.insert(0,first_row)\n",
    "        t = AsciiTable( attention_probs )\n",
    "        print( t.table )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "63009f7f2cc996fccde2458c7f62140de231233c09c29bb8f0fa7c6864762ba3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
